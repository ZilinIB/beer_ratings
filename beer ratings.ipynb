{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"beer ratings.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"G7Hjt9j3m_K8","colab_type":"code","outputId":"510e519d-62ff-4b8d-fc22-a33ab45039ed","executionInfo":{"status":"ok","timestamp":1563739793747,"user_tz":-120,"elapsed":5573,"user":{"displayName":"Zilin Wang","photoUrl":"https://lh6.googleusercontent.com/-axH6JTJVgvw/AAAAAAAAAAI/AAAAAAAAABQ/iSYknUJvTeM/s64/photo.jpg","userId":"17196222863736081009"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!pip install contractions textsearch"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.21)\n","Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (0.0.17)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.4.0)\n","Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.1.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-WHfVil8Y5IQ","colab_type":"code","colab":{}},"source":["# TensorFlow\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Embedding, BatchNormalization, Reshape, Concatenate, Flatten\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","# Helper libraries\n","import pandas as pd\n","import numpy as np\n","import math\n","\n","# Preprocessing data\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n","\n","\n","# NLP denpendencies\n","import nltk\n","from nltk.tokenize import word_tokenize\n","import contractions\n","import re\n","\n","# Maximal length of a review text\n","MAX_LEN = 250"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXgsut4vzMsJ","colab_type":"code","outputId":"21218164-c390-4b7e-dfdf-fdbb4679947d","executionInfo":{"status":"ok","timestamp":1563739793751,"user_tz":-120,"elapsed":5529,"user":{"displayName":"Zilin Wang","photoUrl":"https://lh6.googleusercontent.com/-axH6JTJVgvw/AAAAAAAAAAI/AAAAAAAAABQ/iSYknUJvTeM/s64/photo.jpg","userId":"17196222863736081009"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["nltk.download('stopwords')\n","nltk.download('punkt')\n","stopwords = nltk.corpus.stopwords.words('english')\n","stopwords.remove('no')\n","stopwords.remove('not')"],"execution_count":45,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"574sJH2fV49Y","colab_type":"code","outputId":"2e34c897-ae47-455d-ad8d-53502772be5e","executionInfo":{"status":"ok","timestamp":1563739793751,"user_tz":-120,"elapsed":5501,"user":{"displayName":"Zilin Wang","photoUrl":"https://lh6.googleusercontent.com/-axH6JTJVgvw/AAAAAAAAAAI/AAAAAAAAABQ/iSYknUJvTeM/s64/photo.jpg","userId":"17196222863736081009"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Mount to google drive folder\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o8cQVrLFYkvn","colab_type":"code","colab":{}},"source":["# Training data\n","df_train = pd.read_csv(\"/content/gdrive/My Drive/colab notebook/beer ratings/train.csv\", index_col=['index'])\n","\n","# Divide the values by 5 to match the outputs of sigmoid function in the output layer of the following model\n","y_train = df_train[['review/appearance', 'review/aroma', 'review/overall', 'review/palate', 'review/taste']].applymap(lambda x: x / 5)\n","\n","# Test data\n","df_test = pd.read_csv(\"/content/gdrive/My Drive/colab notebook/beer ratings/test.csv\", index_col=['index'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DuVcaOBzk4uT","colab_type":"code","colab":{}},"source":["def build_model(len_embed_cols, num_words):\n","    \n","  model_out = []\n","  model_in  = []\n","\n","  # Embedding inputs for categorical features\n","  for dim in len_embed_cols:\n","      input_dim = Input(shape=(1,), dtype='int32')\n","      embed_dim = Embedding(dim, max(2, int(math.log(dim))), input_length=1)(input_dim)\n","      embed_dim = Dropout(0.25)(embed_dim)\n","      embed_dim = Reshape((max(2, int(math.log(dim))),))(embed_dim)\n","      model_out.append(embed_dim)\n","      model_in.append(input_dim)\n","\n","  # Numerical features\n","  input_numeric = Input(shape=(3,), dtype='float32')\n","  model_in.append(input_numeric)\n","  \n","  # Text feature\n","  input_text = Input(shape=(MAX_LEN, ), dtype='int32')\n","  embed_text = Embedding(num_words, 40, input_length=MAX_LEN)(input_text)\n","  embed_text = Dropout(0.25)(embed_text)\n","  embed_text = Flatten()(embed_text)\n","  embed_text = Dense(6)(embed_text)\n","  embed_text = Activation('relu')(embed_text)\n","  model_out.append(embed_text)\n","  model_in.append(input_text)\n","\n","  # Combining the output of embedding model and numerical features as the input of the final model\n","  outputs = Concatenate(axis=1)([*model_out, input_numeric])\n","  outputs = Dense(512)(outputs) \n","  outputs = BatchNormalization()(outputs)\n","  outputs = Activation('relu')(outputs)\n","  outputs = Dropout(0.5)(outputs)\n","  outputs = Dense(256)(outputs) \n","  outputs = BatchNormalization()(outputs)\n","  outputs = Activation('relu')(outputs)\n","  outputs = Dropout(0.5)(outputs)\n","  outputs = Dense(128)(outputs) \n","  outputs = BatchNormalization()(outputs)\n","  outputs = Activation('relu')(outputs)\n","  outputs = Dropout(0.5)(outputs)\n","  outputs = Dense(5)(outputs)\n","  outputs = Activation('sigmoid')(outputs)\n","\n","  model = Model(model_in, outputs)\n","\n","  model.compile(optimizer='sgd', \n","                loss='mse',\n","                metrics=['mae', 'acc'])\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9taJLT57n9mp","colab_type":"code","colab":{}},"source":["def normalize_text(text):\n","  # Extract the text and lower the case\n","  text = str(text).lower()\n","  # Expand contractions\n","  text = contractions.fix(text)\n","  # Remove redundant spaces\n","  text = re.sub('[\\s+]', ' ', text)\n","  # Remove special characters and numbers\n","  text = re.sub('[^a-zA-z\\s]', '', text)\n","  # Tokenize the text\n","  tokens = word_tokenize(text)\n","  # Remove stopwords\n","  words = [token for token in tokens if token not in stopwords]\n","  # Join the words\n","  normalized_text = ' '.join(words)\n","  return normalized_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6GeSVI1Kd2W","colab_type":"code","colab":{}},"source":["def process_cat_cols(X_train, X_test, cat_cols):\n","  # We use entity embedding to deal with the categorical features\n","  col_vals_dict = {c: list(X_train[c].unique()) for c in cat_cols}\n","  print(col_vals_dict.keys())\n","\n","  # LabelEncoder\n","  for c in cat_cols:\n","    lbl = LabelEncoder()\n","    lbl.fit(list(X_train[c].values) + list(X_test[c].values))\n","    X_train[c] = lbl.transform(list(X_train[c].values))\n","    X_test[c] = lbl.transform(list(X_test[c].values))\n","\n","  len_embed_cols = []\n","  for c in cat_cols:\n","    len_embed_cols.append(len(col_vals_dict[c]))\n","    print(c + ': %d values' % len(col_vals_dict[c])) #look at value counts to know the embedding dimensions\n","\n","  print('\\nNumber of categorical features :', len(cat_cols))\n","  \n","  return len_embed_cols\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0wVy8AXMMA7","colab_type":"code","colab":{}},"source":["def process_numeric_cols(X_train, X_test, numeric_cols):\n","  # standardize beer/ABV, review/timeUnix, user/ageInSeconds\n","  scaler = StandardScaler()\n","  X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n","  X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n","  \n","  print('\\nNumber of numerical features :', len(numeric_cols))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PeVj5wgMsZQ","colab_type":"code","colab":{}},"source":["def process_text_col(X_train, X_test, text_col):\n","  # Extract the texts\n","  reviews_train = df_train[text_col]\n","  reviews_test = df_test[text_col]\n","\n","  # Normalize text\n","  reviews_lines_train = reviews_train.map(normalize_text)\n","  reviews_lines_test = reviews_test.map(normalize_text)\n","\n","  # Tokenize the text\n","  tokenizer = Tokenizer()\n","  tokenizer.fit_on_texts(reviews_lines_train)\n","  words_seq_train = tokenizer.texts_to_sequences(reviews_lines_train)\n","  words_seq_test = tokenizer.texts_to_sequences(reviews_lines_test)\n","  \n","  # Padding the sequence to MAX_LEN with 0 or truncate it if its length exceeds the limit\n","  words_seq_train_padded = pad_sequences(words_seq_train, maxlen=MAX_LEN)\n","  words_seq_test_padded = pad_sequences(words_seq_test, maxlen=MAX_LEN)\n","  num_words = len(tokenizer.word_index) + 1\n","  print('\\nVocabulary length :', num_words)\n","  \n","  return num_words, words_seq_train_padded, words_seq_test_padded"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vc7dZa_7jGgS","colab_type":"code","colab":{}},"source":["def preprocess_data(df_train, df_test):\n","  # Remove columns:\n","  #  beer/name: brewerId together with beer/style approximately indicates the beer\n","  #  review/appearance, review/aroma, review/overall, review/palate, review/taste: columns to predict on test dataset\n","  #  review/timeStruct: redundant to review/timeUnix\n","  #  user/birthdayRaw, user/birthdayUnix: redundant to user/ageInSeconds\n","  X_train = df_train.drop(columns=['beer/name', 'review/appearance', 'review/aroma', 'review/overall', 'review/palate', 'review/taste', \n","                   'review/timeStruct', 'user/birthdayRaw', 'user/birthdayUnix'])\n","  X_test = df_test.drop(columns=['beer/name', 'review/appearance', 'review/aroma', 'review/overall', 'review/palate', 'review/taste', \n","                   'review/timeStruct', 'user/birthdayRaw', 'user/birthdayUnix'])\n","  \n","  # Input data for the NN model\n","  input_train = []\n","  input_test = [] \n","  \n","  ### Process categorical features for the model input\n","  cat_cols = ['beer/beerId', 'beer/brewerId', 'beer/style', 'user/profileName', 'user/gender']\n","  # Fill the empty entries in gender with 'Unknown'\n","  X_train['user/gender'].fillna('Unknown', inplace=True)\n","  X_test['user/gender'].fillna('Unknown', inplace=True)\n","   \n","  len_embed_cols = process_cat_cols(X_train, X_test, cat_cols) \n","  # Columns to be embedded: rescaling to range [0, # values)\n","  for c in cat_cols:\n","    input_train.append(X_train[c].values)\n","    input_test.append(X_test[c].values)\n","  \n","  ### Process numerical features\n","  numeric_cols = ['beer/ABV', 'review/timeUnix', 'user/ageInSeconds']\n","  # Fill the entries in age column with the average age\n","  mean_age = X_train['user/ageInSeconds'].mean()\n","  X_train['user/ageInSeconds'].fillna(mean_age, inplace=True)\n","  X_test['user/ageInSeconds'].fillna(mean_age, inplace=True)\n","  \n","  process_numeric_cols(X_train, X_test, numeric_cols)\n","  \n","  input_train.append(X_train[numeric_cols].values)\n","  input_test.append(X_test[numeric_cols].values)\n","  \n","  ### Process review/text \n","  text_col = 'review/text'\n","  \n","  num_words, words_seq_train_padded, words_seq_test_padded = process_text_col(X_train, X_test, text_col)\n","    \n","  input_train.append(words_seq_train_padded)\n","  input_test.append(words_seq_test_padded)\n","\n","  return input_train, input_test, len_embed_cols, num_words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUhW7T23okA4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"6f52c224-62d4-4c9f-e0cc-4a32733a52cc","executionInfo":{"status":"ok","timestamp":1563739860152,"user_tz":-120,"elapsed":71769,"user":{"displayName":"Zilin Wang","photoUrl":"https://lh6.googleusercontent.com/-axH6JTJVgvw/AAAAAAAAAAI/AAAAAAAAABQ/iSYknUJvTeM/s64/photo.jpg","userId":"17196222863736081009"}}},"source":["# Preprocess data\n","input_train, input_test, len_embed_cols, num_words = preprocess_data(df_train, df_test)\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["dict_keys(['beer/beerId', 'beer/brewerId', 'beer/style', 'user/profileName', 'user/gender'])\n","beer/beerId: 1731 values\n","beer/brewerId: 212 values\n","beer/style: 95 values\n","user/profileName: 7442 values\n","user/gender: 3 values\n","\n","Number of categorical features : 5\n","\n","Number of numerical features : 3\n","\n","Vocabulary length : 59191\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c_kKPV_3tLcX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7ab83dcb-f86d-4f69-8a43-db70bccbbc8e"},"source":["# Create model and train the NN\n","\n","model = build_model(len_embed_cols, num_words)\n","\n","model.fit(input_train, y_train,\n","\tepochs=400,\n","\tbatch_size=32,\n","\tvalidation_split=0.1,\n","  callbacks = [EarlyStopping(monitor='val_loss', patience=50)]\n",")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 33750 samples, validate on 3750 samples\n","Epoch 1/400\n","33750/33750 [==============================] - 17s 494us/sample - loss: 0.0646 - mean_absolute_error: 0.1955 - acc: 0.2198 - val_loss: 0.0242 - val_mean_absolute_error: 0.1291 - val_acc: 0.1819\n","Epoch 2/400\n","33750/33750 [==============================] - 16s 480us/sample - loss: 0.0390 - mean_absolute_error: 0.1527 - acc: 0.2132 - val_loss: 0.0194 - val_mean_absolute_error: 0.1112 - val_acc: 0.1872\n","Epoch 3/400\n","33750/33750 [==============================] - 17s 490us/sample - loss: 0.0368 - mean_absolute_error: 0.1484 - acc: 0.2114 - val_loss: 0.0190 - val_mean_absolute_error: 0.1093 - val_acc: 0.1872\n","Epoch 4/400\n","33750/33750 [==============================] - 16s 469us/sample - loss: 0.0350 - mean_absolute_error: 0.1447 - acc: 0.2151 - val_loss: 0.0187 - val_mean_absolute_error: 0.1078 - val_acc: 0.1891\n","Epoch 5/400\n","33750/33750 [==============================] - 17s 491us/sample - loss: 0.0338 - mean_absolute_error: 0.1421 - acc: 0.2100 - val_loss: 0.0186 - val_mean_absolute_error: 0.1077 - val_acc: 0.1795\n","Epoch 6/400\n","33750/33750 [==============================] - 17s 507us/sample - loss: 0.0330 - mean_absolute_error: 0.1402 - acc: 0.2111 - val_loss: 0.0186 - val_mean_absolute_error: 0.1072 - val_acc: 0.1853\n","Epoch 7/400\n","33750/33750 [==============================] - 17s 489us/sample - loss: 0.0320 - mean_absolute_error: 0.1379 - acc: 0.2118 - val_loss: 0.0183 - val_mean_absolute_error: 0.1063 - val_acc: 0.1872\n","Epoch 8/400\n","33750/33750 [==============================] - 17s 490us/sample - loss: 0.0311 - mean_absolute_error: 0.1360 - acc: 0.2156 - val_loss: 0.0185 - val_mean_absolute_error: 0.1069 - val_acc: 0.1888\n","Epoch 9/400\n","33750/33750 [==============================] - 17s 489us/sample - loss: 0.0302 - mean_absolute_error: 0.1339 - acc: 0.2090 - val_loss: 0.0181 - val_mean_absolute_error: 0.1056 - val_acc: 0.1840\n","Epoch 10/400\n","33750/33750 [==============================] - 16s 481us/sample - loss: 0.0294 - mean_absolute_error: 0.1319 - acc: 0.2116 - val_loss: 0.0181 - val_mean_absolute_error: 0.1052 - val_acc: 0.1856\n","Epoch 11/400\n","33750/33750 [==============================] - 17s 511us/sample - loss: 0.0286 - mean_absolute_error: 0.1300 - acc: 0.2092 - val_loss: 0.0179 - val_mean_absolute_error: 0.1046 - val_acc: 0.1869\n","Epoch 12/400\n","33750/33750 [==============================] - 18s 529us/sample - loss: 0.0279 - mean_absolute_error: 0.1284 - acc: 0.2132 - val_loss: 0.0177 - val_mean_absolute_error: 0.1038 - val_acc: 0.1829\n","Epoch 13/400\n","33750/33750 [==============================] - 17s 499us/sample - loss: 0.0272 - mean_absolute_error: 0.1265 - acc: 0.2099 - val_loss: 0.0179 - val_mean_absolute_error: 0.1046 - val_acc: 0.1800\n","Epoch 14/400\n","33750/33750 [==============================] - 17s 497us/sample - loss: 0.0267 - mean_absolute_error: 0.1250 - acc: 0.2161 - val_loss: 0.0176 - val_mean_absolute_error: 0.1036 - val_acc: 0.1856\n","Epoch 15/400\n","33750/33750 [==============================] - 18s 519us/sample - loss: 0.0259 - mean_absolute_error: 0.1234 - acc: 0.2178 - val_loss: 0.0175 - val_mean_absolute_error: 0.1033 - val_acc: 0.1859\n","Epoch 16/400\n","33750/33750 [==============================] - 16s 484us/sample - loss: 0.0256 - mean_absolute_error: 0.1225 - acc: 0.2148 - val_loss: 0.0175 - val_mean_absolute_error: 0.1037 - val_acc: 0.1789\n","Epoch 17/400\n","33750/33750 [==============================] - 17s 499us/sample - loss: 0.0249 - mean_absolute_error: 0.1207 - acc: 0.2161 - val_loss: 0.0172 - val_mean_absolute_error: 0.1024 - val_acc: 0.1835\n","Epoch 18/400\n","33750/33750 [==============================] - 16s 487us/sample - loss: 0.0243 - mean_absolute_error: 0.1191 - acc: 0.2167 - val_loss: 0.0174 - val_mean_absolute_error: 0.1032 - val_acc: 0.1768\n","Epoch 19/400\n","33750/33750 [==============================] - 17s 499us/sample - loss: 0.0239 - mean_absolute_error: 0.1183 - acc: 0.2146 - val_loss: 0.0173 - val_mean_absolute_error: 0.1029 - val_acc: 0.1771\n","Epoch 20/400\n","33750/33750 [==============================] - 16s 485us/sample - loss: 0.0234 - mean_absolute_error: 0.1169 - acc: 0.2204 - val_loss: 0.0173 - val_mean_absolute_error: 0.1031 - val_acc: 0.1795\n","Epoch 21/400\n","33750/33750 [==============================] - 16s 482us/sample - loss: 0.0229 - mean_absolute_error: 0.1157 - acc: 0.2161 - val_loss: 0.0172 - val_mean_absolute_error: 0.1031 - val_acc: 0.1797\n","Epoch 22/400\n","33750/33750 [==============================] - 16s 478us/sample - loss: 0.0225 - mean_absolute_error: 0.1145 - acc: 0.2142 - val_loss: 0.0172 - val_mean_absolute_error: 0.1028 - val_acc: 0.1808\n","Epoch 23/400\n","33750/33750 [==============================] - 17s 499us/sample - loss: 0.0220 - mean_absolute_error: 0.1134 - acc: 0.2152 - val_loss: 0.0174 - val_mean_absolute_error: 0.1041 - val_acc: 0.1787\n","Epoch 24/400\n","33750/33750 [==============================] - 17s 515us/sample - loss: 0.0217 - mean_absolute_error: 0.1127 - acc: 0.2168 - val_loss: 0.0172 - val_mean_absolute_error: 0.1033 - val_acc: 0.1797\n","Epoch 25/400\n","33750/33750 [==============================] - 17s 514us/sample - loss: 0.0213 - mean_absolute_error: 0.1114 - acc: 0.2210 - val_loss: 0.0173 - val_mean_absolute_error: 0.1035 - val_acc: 0.1837\n","Epoch 26/400\n","33750/33750 [==============================] - 18s 520us/sample - loss: 0.0210 - mean_absolute_error: 0.1106 - acc: 0.2205 - val_loss: 0.0172 - val_mean_absolute_error: 0.1031 - val_acc: 0.1773\n","Epoch 27/400\n","33750/33750 [==============================] - 17s 512us/sample - loss: 0.0208 - mean_absolute_error: 0.1102 - acc: 0.2212 - val_loss: 0.0172 - val_mean_absolute_error: 0.1034 - val_acc: 0.1765\n","Epoch 28/400\n","33750/33750 [==============================] - 17s 516us/sample - loss: 0.0204 - mean_absolute_error: 0.1088 - acc: 0.2200 - val_loss: 0.0174 - val_mean_absolute_error: 0.1045 - val_acc: 0.1853\n","Epoch 29/400\n","19744/33750 [================>.............] - ETA: 6s - loss: 0.0199 - mean_absolute_error: 0.1080 - acc: 0.2226"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eYzGEYq6hNwJ","colab_type":"code","colab":{}},"source":["# Make prediction on the test data and multiply them by 5 to match the ratings, write the output in corresponding format\n","y_pred = np.vectorize(lambda x: 5 * x)(model.predict(input_test))\n","index = pd.DataFrame(data=df_test.index.values, columns=['index'])\n","result = pd.concat([index, pd.DataFrame(y_pred, columns=['review/appearance', 'review/aroma', 'review/overall', 'review/palate', 'review/taste'])], axis=1)\n","result.to_csv(\"/content/gdrive/My Drive/colab notebook/beer ratings/result.csv\", index = False)\n"],"execution_count":0,"outputs":[]}]}